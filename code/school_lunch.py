import requests
import pandas as pd
from datetime import datetime, timedelta
import pyarrow
import json
import re
import tempfile
import os
from pyspark.sql import SparkSession
from pyspark.sql.types import StructType, StructField, StringType, FloatType
import sys

webhook_url = 'https://discordapp.com/api/webhooks/1362586291937612107/gXsqabc7FDZLsmEk23TwXINH89Q1m9zZb9pDevUEFopdePsjcyCEwiBYIIcwloSrKrnz'
DATE = sys.argv[1]
os.environ["GOOGLE_APPLICATION_CREDENTIALS"] = "/home/ubuntu/.ssh/shining-reality-455501-q0-7b280468bf04.json"

from google.cloud import storage


# gcsì—ì„œ í•™êµ ì½”ë“œ json íŒŒì¼ì„ ì½ì–´ì˜¤ê¸°
def load_json_from_gcs(bucket_name: str, blob_name: str):
    client = storage.Client()
    print(f"Downloading {blob_name} from {bucket_name}")
    bucket = client.bucket(bucket_name)
    blob = bucket.blob(blob_name)

    json_str = blob.download_as_text(encoding='utf-8')
    print("Download complete")
    data = json.loads(json_str)
    df = pd.DataFrame(data)
    return df

# apiì—ì„œ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¤ê¸°
def get_api():
    df = load_json_from_gcs("school-lunch-bucket", "lunch_menu/school_data.json")
    # í•„ìš”í•œ ì»¬ëŸ¼ë§Œ ì¶”ì¶œ
    school_df = df[['ATPT_OFCDC_SC_CODE', 'SD_SCHUL_CODE', 'SCHUL_KND_SC_NM']].copy()
    # ì»¬ëŸ¼ëª… í•œê¸€ë¡œ ë³€ê²½ (ì„ íƒ)
    school_df.columns = ['ì§€ì—­ì½”ë“œ', 'í•™êµì½”ë“œ', 'í•™êµêµ¬ë¶„']
    
    # SparkSession ìƒì„±
    spark = SparkSession.builder.appName("school-lunch").getOrCreate()
    
    # í™˜ê²½ì„¤ì •
    API_KEY = '261957623ead45779884d5b6e27385cf' # ë³¸ì¸ API_KEY ì…ë ¥
    EDU_CODE = school_df['ì§€ì—­ì½”ë“œ'].unique()
    SCH_CODE = []
    BASE_URL = 'https://open.neis.go.kr/hub/mealServiceDietInfo'

    # ìµœì¢… DataFrame
    all_df = pd.DataFrame()

    # ğŸ‘‰ ë‚ ì§œ ë°˜ë³µ
    ymd = DATE
    page = 1
    
    url_list = []
    for reg in EDU_CODE:
        SCH_CODE = school_df[school_df['ì§€ì—­ì½”ë“œ'].str.upper() == reg]['í•™êµì½”ë“œ']
        for school in SCH_CODE:
            url_list.append(f'https://open.neis.go.kr/hub/mealServiceDietInfo?KEY={API_KEY}&Type=json&ATPT_OFCDC_SC_CODE={reg}&SD_SCHUL_CODE={school}&MLSV_YMD={ymd}&MMEAL_SC_CODE=2&pIndex={page}&pSize=1000')
    
    urls = spark.sparkContext.parallelize(url_list, numSlices=20)
    print(f"ì´ URL ìˆ˜: {len(url_list)}")
    
    rdd = urls.flatMap(fetch_json)
    all_df = spark.createDataFrame(rdd)
    all_df = all_df.toPandas()
        
    # í•™êµ êµ¬ë¶„ ì¶”ê°€
    school_merge_df = school_df[['í•™êµì½”ë“œ', 'í•™êµêµ¬ë¶„']]

    # ë³‘í•©
    all_df = all_df.merge(school_merge_df, how='left', left_on='SD_SCHUL_CODE', right_on='í•™êµì½”ë“œ')
    # 'LV' ì»¬ëŸ¼ìœ¼ë¡œ ì´ë¦„ ë³€ê²½
    all_df.rename(columns={'í•™êµêµ¬ë¶„': 'LV'}, inplace=True)
    
    message = {
    "content": f"ì „êµ­ {ymd} ë°ì´í„° ì¶”ì¶œ ì„±ê³µ"
    }
    response = requests.post(webhook_url, data=json.dumps(message), headers={'Content-Type': 'application/json'})
    print('--------------------------------------------------------------------------------')
    print("1ì°¨ : ë°ì´í„° ì²˜ë¦¬ ì„±ê³µ")
    print('--------------------------------------------------------------------------------')
    
    return all_df

# SPARKì²˜ë¦¬ìš© í•¨ìˆ˜
def fetch_json(url):
    try:
        res = requests.get(url)
        res.raise_for_status()  # HTTP ì—ëŸ¬ ì½”ë“œ ì²´í¬ (4xx, 5xx)
        data = res.json()
        if "mealServiceDietInfo" in data and len(data["mealServiceDietInfo"]) > 1:
            return data["mealServiceDietInfo"][1]["row"]  # âœ… í•µì‹¬ ë°ì´í„° ë°˜í™˜
        else:
            return []  # ë°ì´í„°ê°€ ì—†ì„ ê²½ìš° ë¹ˆ ë¦¬ìŠ¤íŠ¸
    except Exception as e:
        print(f"Error fetching {url}: {e}")
        return []


def pre_parquet(df):

    # í•„ìš”í•œ ì»¬ëŸ¼ë§Œ ìœ ì§€
    keep_cols = [
        "MLSV_YMD",           # ê¸‰ì‹ ë‚ ì§œ
        "ATPT_OFCDC_SC_CODE", # êµìœ¡ì²­ ì½”ë“œ
        "ATPT_OFCDC_SC_NM",   # êµìœ¡ì²­ ì´ë¦„
        "SD_SCHUL_CODE",      # í•™êµ ì½”ë“œ
        "SCHUL_NM",           # í•™êµ ì´ë¦„
        "DDISH_NM",           # ê¸‰ì‹ ë©”ë‰´
        "CAL_INFO",           # ì¹¼ë¡œë¦¬ ì •ë³´
        "NTR_INFO",           # ì˜ì–‘ì†Œ ì •ë³´
        "MLSV_FGR",           # ê¸‰ì‹ ì¸ì› ìˆ˜
        "LV"                  # í•™êµ ì¢…ë¥˜
    ]
    df = df[keep_cols]
    
    # ë‚ ì§œ í˜•ì‹ ë³€í™˜
    df["DATE"] = pd.to_datetime(df["MLSV_YMD"], format="%Y%m%d")
    
    df = df.rename(columns={
        "ATPT_OFCDC_SC_CODE": "REG_C",  # ì§€ì—­ ì½”ë“œ
        "ATPT_OFCDC_SC_NM": "REG_N",    # êµìœ¡ì²­ ì´ë¦„
        "SD_SCHUL_CODE": "SCH_C",       # í•™êµ ì½”ë“œ
        "SCHUL_NM": "SCH_N",            # í•™êµ ì´ë¦„
        "MLSV_FGR": "COUNT",            # ê¸‰ì‹ ì¸ì› ìˆ˜
        })


    # ì¹¼ë¡œë¦¬ ìˆ˜ì¹˜ ì¶”ì¶œ
    df["CAL"] = df["CAL_INFO"].str.extract(r"([\d.]+)").astype(float)

    # ì˜ì–‘ì •ë³´ dict ë³€í™˜
    def parse_nutrition(info):
        try:
            parts = info.split("<br/>")
            return {
                kv.split(":")[0].strip(): kv.split(":")[1].strip()
                for kv in parts if ":" in kv
            }
        except:
            return {}

    df["NUT_DICT"] = df["NTR_INFO"].apply(parse_nutrition)

    # ë©”ë‰´ íŒŒì‹± ë° ì»¬ëŸ¼í™”
    def parse_menu_list(dish_text):
        try:
            return [item.strip() for item in dish_text.split("<br/>") if item.strip()]
        except:
            return []
        
    def clean_menu(menu_item):
        # í•œê¸€/ìˆ«ìë§Œ ì²˜ìŒë¶€í„° ëê¹Œì§€ ì¶”ì¶œ (ì˜ë¬¸ìë‚˜ íŠ¹ìˆ˜ë¬¸ì ë‚˜ì˜¤ê¸° ì „ê¹Œì§€ë§Œ)
        match = re.match(r'^[ê°€-í£0-9]+', menu_item)
        return match.group(0) if match else menu_item
    
    def remove_trailing_digits(menu_item):
        # ë’¤ì— ë¶™ì€ ìˆ«ìë§Œ ì œê±° (ì¤‘ê°„ ìˆ«ìëŠ” ìœ ì§€)
        return re.sub(r'\d+$', '', menu_item)
    
    df["MENU_LIST"] = df["DDISH_NM"].apply(parse_menu_list)
    df["MENU"] = df["MENU_LIST"].apply(lambda menus: [clean_menu(m) for m in menus])
    df["MENU"] = df["MENU"].apply(lambda menus: [remove_trailing_digits(m) for m in menus])
    df["MENU"] = df["MENU"].apply(list)
    rdf = df.explode("MENU").reset_index(drop=True)

    # ì •ë¦¬: ì›ë³¸ í…ìŠ¤íŠ¸ ì»¬ëŸ¼ ì œê±°
    rdf.drop(columns=["MLSV_YMD", "CAL_INFO", "NTR_INFO", "DDISH_NM", "MENU_LIST"], inplace=True)
    print('--------------------------------------------------------------------------------')
    print("2ì°¨ : ë°ì´í„° ì „ì²˜ë¦¬ ì„±ê³µ")
    print('--------------------------------------------------------------------------------')
    return rdf


def upload_partitioned_parquet_to_gcs(df, bucket_name, base_path):
    df['DATE'] = pd.to_datetime(df['DATE'])
    df['DATE_YEAR'] = df['DATE'].dt.year.astype('int64')
    df['DATE_MONTH'] = df['DATE'].dt.month.astype('int64')

    # ì„ì‹œ ë””ë ‰í† ë¦¬ì— partition ì €ì¥
    with tempfile.TemporaryDirectory() as tmp_dir:
        df.to_parquet(tmp_dir, engine='pyarrow', index=False, partition_cols=['DATE_YEAR', 'DATE_MONTH'])

        client = storage.Client()
        bucket = client.bucket(bucket_name)

        # í´ë” ë‚´ ëª¨ë“  íŒŒì¼ì„ GCSë¡œ ì—…ë¡œë“œ
        for root, _, files in os.walk(tmp_dir):
            for file in files:
                local_path = os.path.join(root, file)
                relative_path = os.path.relpath(local_path, tmp_dir)
                blob_path = os.path.join(base_path, relative_path).replace("\\", "/")  # Windows ê²½ë¡œ ëŒ€ì‘
                blob = bucket.blob(blob_path)
                blob.upload_from_filename(local_path)
                print(f"âœ… Uploaded to gs://{bucket_name}/{blob_path}")
    
    print('--------------------------------------------------------------------------------')
    print("3ì°¨ : ë°ì´í„° ì—…ë¡œë“œ ì„±ê³µ")
    print('--------------------------------------------------------------------------------')

# ë©”ì¸ ì‹¤í–‰
df = get_api()
rdf = pre_parquet(df)
upload_partitioned_parquet_to_gcs(rdf, 'school-lunch-bucket', 'lunch_menu')
